{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cupy as cp\n",
    "from cuml.decomposition import TruncatedSVD\n",
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReducedEmbeddingModel:\n",
    "    def __init__(self, vectors, index_to_key):\n",
    "        self.vectors = vectors\n",
    "        self.index_to_key = index_to_key\n",
    "        self.key_to_index = {word: idx for idx, word in enumerate(index_to_key)}\n",
    "    \n",
    "    def __getitem__(self, word):\n",
    "        if word in self.key_to_index:\n",
    "            return self.vectors[self.key_to_index[word]]\n",
    "        raise KeyError(f\"Word '{word}' not found in vocabulary\")\n",
    "    \n",
    "    def __contains__(self, word):\n",
    "        return word in self.key_to_index\n",
    "\n",
    "def load_glove_model(file_path):\n",
    "    \"\"\"Loads GloVe embeddings from a text file.\"\"\"\n",
    "    index_to_key = []\n",
    "    vectors = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.split()\n",
    "            word = parts[0]\n",
    "            vector = np.array(parts[1:], dtype=np.float32)\n",
    "            index_to_key.append(word)\n",
    "            vectors.append(vector)\n",
    "    vectors = np.vstack(vectors)\n",
    "    return ReducedEmbeddingModel(vectors, index_to_key)\n",
    "    \n",
    "# Load SimLex-999 word similarity data\n",
    "def load_simlex_999(file_path):\n",
    "    \"\"\"Loads SimLex-999 and returns word pairs with similarity scores.\"\"\"\n",
    "    data = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        next(file)  # Skip header\n",
    "        for line in file:\n",
    "            parts = line.strip().split('\\t')\n",
    "            word1, word2, similarity_score = parts[0], parts[1], float(parts[3])\n",
    "            data.append((word1, word2, similarity_score))\n",
    "    return data\n",
    "\n",
    "# Cosine similarity function for similarity evaluation\n",
    "def cosine_similarity(v1, v2):\n",
    "    norm1 = np.linalg.norm(v1)\n",
    "    norm2 = np.linalg.norm(v2)\n",
    "    if norm1 == 0 or norm2 == 0:\n",
    "        return 0\n",
    "    return np.dot(v1, v2) / (norm1 * norm2)\n",
    "\n",
    "# Evaluate embeddings on the word similarity data\n",
    "def evaluate_embeddings(model, word_pairs):\n",
    "    \n",
    "    predicted_scores = []\n",
    "    true_scores = []\n",
    "\n",
    "    model_dict = {}\n",
    "    for i in range(len(model.index_to_key)):\n",
    "        model_dict[model.index_to_key[i]] = model.vectors[i]\n",
    "    \n",
    "    for word1, word2, true_score in word_pairs:\n",
    "        if word1 in model_dict and word2 in model_dict:\n",
    "            sim_score = cosine_similarity(model_dict[word1], model_dict[word2])\n",
    "            predicted_scores.append(sim_score)\n",
    "            true_scores.append(true_score)\n",
    "    \n",
    "    # Calculate Spearman correlation for word similarity performance\n",
    "    correlation, _ = spearmanr(true_scores, predicted_scores)\n",
    "    return correlation\n",
    "\n",
    "def task_driven_dimensionality_reduction(embeddings, target_dim, word_pairs, model):\n",
    "    \"\"\"\n",
    "    Reduces dimensions using TruncatedSVD with optimization for similarity, similar to PCA approach.\n",
    "    \"\"\"\n",
    "    cu_embeddings = cp.asarray(embeddings)\n",
    "    original_dim = cu_embeddings.shape[1]\n",
    "    \n",
    "    best_correlation = -1.0\n",
    "    best_embeddings = None\n",
    "    results = []\n",
    "\n",
    "    # Perform initial TruncatedSVD to a larger dimension to capture more variance initially\n",
    "    initial_n_components = min(original_dim, 300)  # Example: Reduce to 300 initially\n",
    "    svd = TruncatedSVD(n_components=initial_n_components, algorithm=\"full\")\n",
    "    reduced_embeddings = svd.fit_transform(cu_embeddings)\n",
    "\n",
    "    for num_remove in range(initial_n_components - target_dim + 1):\n",
    "        # Remove top components\n",
    "        temp_embeddings = reduced_embeddings[:, num_remove:]\n",
    "\n",
    "        # Further reduce to target_dim if necessary\n",
    "        if temp_embeddings.shape[1] > target_dim:\n",
    "            svd_final = TruncatedSVD(n_components=target_dim, algorithm=\"full\")\n",
    "            final_embeddings = svd_final.fit_transform(temp_embeddings)\n",
    "        else:\n",
    "            final_embeddings = temp_embeddings\n",
    "        \n",
    "        final_embeddings_np = cp.asnumpy(final_embeddings)\n",
    "\n",
    "        # Create a reduced model for evaluation\n",
    "        reduced_model = ReducedEmbeddingModel(final_embeddings_np, model.index_to_key)\n",
    "\n",
    "        # Evaluate\n",
    "        correlation = evaluate_embeddings(reduced_model, word_pairs)\n",
    "        results.append((num_remove, correlation))\n",
    "        print(f\"Removed top {num_remove} components, new dim = {final_embeddings.shape[1]}, correlation = {correlation:.4f}\")\n",
    "\n",
    "        if correlation > best_correlation:\n",
    "            best_correlation = correlation\n",
    "            best_embeddings = final_embeddings_np\n",
    "\n",
    "    print(f\"\\nBest correlation: {best_correlation:.4f}\")\n",
    "    return best_embeddings, best_correlation, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_file_path = '/teamspace/uploads/glove.6B.300d.txt'  # Update this path to your GloVe file\n",
    "simlex_file_path = '/teamspace/studios/pca/SimLex-999.txt'  # Update this path to your SimLex-999 file\n",
    "target_dim = 2\n",
    "\n",
    "print(\"Loading GloVe model...\")\n",
    "original_model = load_glove_model(glove_file_path)\n",
    "\n",
    "# Load word similarity dataset\n",
    "word_pairs = load_simlex_999(simlex_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform task-driven dimensionality reduction with TruncatedSVD and top component removal\n",
    "optimized_embeddings, best_correlation, results = task_driven_dimensionality_reduction(\n",
    "    original_model.vectors,\n",
    "    target_dim=target_dim,\n",
    "    word_pairs=word_pairs,\n",
    "    model=original_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Save the 2D embeddings with word labels to a CSV file\n",
    "def save_embeddings_to_csv(words, embeddings, file_path):\n",
    "    print(f\"Saving 2D embeddings to {file_path}...\")\n",
    "    df = pd.DataFrame(embeddings, columns=['x', 'y'])\n",
    "    df['word'] = words\n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(\"2D embeddings saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get words and their high-dimensional embeddings\n",
    "words = original_model.index_to_key  # List of words in vocabulary\n",
    "file_path=\"Glove_TSVD_2d_word_embeddings.csv\"\n",
    "# Save the embeddings and words to a CSV file\n",
    "save_embeddings_to_csv(words, optimized_embeddings, file_path)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
